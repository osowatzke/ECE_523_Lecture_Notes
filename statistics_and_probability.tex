\documentclass[fleqn]{article}
\usepackage[nodisplayskipstretch]{setspace}
\usepackage{amsmath, nccmath, bm}
\usepackage{amssymb}

\newcommand{\zerodisplayskip}{
	\setlength{\abovedisplayskip}{0pt}%
	\setlength{\belowdisplayskip}{0pt}%
	\setlength{\abovedisplayshortskip}{0pt}%
	\setlength{\belowdisplayshortskip}{0pt}%
	\setlength{\mathindent}{0pt}}
	
\title{Statistics and Probability}
\author{}
\date{}

\setlength{\parindent}{0pt}

\begin{document}
	\offinterlineskip
	\setlength{\lineskip}{12pt}
	\zerodisplayskip
	\maketitle
	
	A \underline{continuous random variable} $x$ can take on a continuum of values over some interval $(a,b)$ or over sets of intervals.
	
	The \underline{cummulative distribution function} (CDF) of a random variable $x$ is defined as
	
	\begin{equation*}
		F_X(a) \overset{\Delta}{=} \text{Probability}\{x \leq a\}
	\end{equation*}
	
	$F(a)$ is non-decreasing, $F(-\infty) = 0, F(\infty) = 1$
	
	The \underline{probability density function} (pdf) of a continuous random variable is defined as
	
	\begin{equation*}
		p(x) \overset{\Delta}{=} \frac{dF(x)}{dx}
	\end{equation*}
	
	$p(x) \geq 0$ and $\int_{-\infty}^{\infty}p(x)dx = 1$
	
	For the discrete random variable, we can use the probability mass function instead of the pdf.
	
	Let $X$ be a random variable with pdf $p_X(x)$. Let $Y = h(X)$. Then, the pdf of $Y$ is given by
	
	\begin{equation*}
		p_Y(y) = p_X(x) \cdot \left|\frac{dx}{dy}\right|
	\end{equation*}
	
	Expected value of $g(x) = E\{g(x)\} \overset{\Delta}{=}\int_{-\infty}^{\infty}g(x)p_X(x)dx$
	
	Mean $ = E\{x\} = \int_{-\infty}^{\infty}{xp_X(x)dx}$
	
	Variance $\sigma_X^2 = E\{x^2\} - E^2\{x\}$
	
	A continuous random variable can also be completely characterized by its \newline \underline{characteristic function}.
	
	\begin{equation*}
		Q_X(u) \overset{\Delta}{=} E\{e^{jux}\} = \int_{-\infty}^{\infty}{e^{jux}p_X(x)dx}
	\end{equation*}
	
	\begin{equation*}
		\therefore p_X(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty}{Q_x(u)e^{-jux}du}
	\end{equation*}
	
	\begin{equation*}
		E\{x^n\} = (-j)^n\cdot\left.\frac{d^nQ_x(u)}{du^n}\right\vert_{u=0}
	\end{equation*}
	
	The \underline{joint CDF} of two random variables $X$ and $Y$ is
	
	$F_{X,Y}(a,b) = \text{Pr}\{X \leq a, Y \leq b\}$
	
	The \underline{joint pdf} is defined according to
	
	\begin{equation*}
		p_{X,Y}(x,y) \overset{\Delta}{=} \frac{\partial^2F_{X,Y}(x,y)}{\partial{x}\partial{y}}
	\end{equation*}
	
	The \underline{marginal pdfs} are given by
	
	\begin{equation*}
		p_X(x) = \int_{-\infty}^{\infty}p_{X,Y}(x,y)dy
	\end{equation*}
	
	\begin{equation*}
		p_Y(y) = \int_{-\infty}^{\infty}p_{X,Y}(x,y)dx
	\end{equation*}
	
	Two random variables are said to be \underline{statistically independent} if
	
	$p_{X,Y}(x,y) = p_X(x)p_Y(y)$
	
	Two random variabels are said to be \underline{uncorrelated} if
	
	$E\{XY\} = E\{X\}E\{Y\}$
	
	Note that statistical independence implies uncorrelatedness, but the converse is not generally true. (For Guassian random variables, statistical independence and uncorrelatedness are equivalent).
	
	$\text{COV}(X,Y) \overset{\Delta}{=} E\{(X-E[X])(Y-E[Y]) \} = E\{XY\} - E\{X\}E\{Y\}$
	
	The correlation coefficient $\rho$ betweeen $X$ and $Y$ is defined as
	
	\begin{equation*}
		\rho_{X,Y} \overset{\Delta}{=} \frac{\text{COV}(X,Y)}{\sigma_X\cdot\sigma_Y}
	\end{equation*}
	
	It can be shown that $-1 \leq \rho \leq 1$
	
	$\rho = +1$ implies a strong positive correlation between the two random variables.
	
	$\rho = 0$ implies no correlation
	
	$\rho = -1$ implies a strong negative correlation	
	
	For $N$ random variables $X_1, X_2,...,X_N$, we have statistical independence if
	
	\begin{equation*}
		p_{X_1, X_2,...,X_N}(x_1,x_2,...,x_N) = \prod_{j=1}^{N}p_{X_j}(x_j)
	\end{equation*}
	
	we can represent $N$ random variables compactly using a \underline{single} column vector
	
	\begin{equation*}
		\mathbf{X} = \begin{bmatrix} X_1, X_2,... ,X_N\end{bmatrix}^T
	\end{equation*}
	
	Then $p_\mathbf{X}(\mathbf{x})$ is a \underline{scalar} valued function of the vector $\mathbf{x}$ denoting the N-variate pdf of the $N$ random variables.
	
	The \underline{mean vector} of the random variable $\mathbf{X}$ is defined according to
	
	$\boldsymbol{\mu} = E\{\mathbf{x}\} \overset{\Delta}{=} \begin{bmatrix}E\{x_1\},E\{x_2\},...,E\{x_N\}\end{bmatrix}^T$
	
	Similarly, the \underline{covariance matrix} $\mathbf{\Sigma}$ is defined as
	
	$\mathbf{\Sigma} \overset{\Delta}{=} E\{(\mathbf{x} - \boldsymbol{\mu})(\mathbf{x} - \boldsymbol{\mu})^T\} = E\{\mathbf{X}\mathbf{X}^T\} - \boldsymbol{\mu}\boldsymbol{\mu}^T$
	
	Note that $\mathbf{\Sigma}$ is a $N \times N$ square, \underline{symmetric} matrix. In addition, it can be shown that $\mathbf{\Sigma}$ is a \underline{positive definite} matrix.
	
	When the $N$ random variables represented by $\mathbf{X}$ are statistically independent, $\mathbf{\Sigma}$ is a \underline{diagonal} matrix with the diagonal terms indicating the variances of the individual random variables.
	
	Let $Y_1, Y_2,...,Y_M$ be the random variables obtained from $X_1,X_2,...,X_N$ through the following \underline{linear} transformation.
	
	\begin{equation*}
		\begin{aligned}
			Y_1 &= a_{11}X_1 + a_{12}X_2 + \cdots + a_{1N}X_N \\
			Y_2 &= a_{21}X_1 + a_{22}X_2 + \cdots + a_{2N}X_N \\
			&\vdots \\
			Y_M &= a_{M1}X_1 + a_{M2}X_2 + \cdots + a_{MN}X_N
		\end{aligned}
	\end{equation*}
	
	Above linear relations can be expressed compactly using matrix notation as
	
	$\mathbf{Y} = \mathbf{AX}$
	
	where $\mathbf{X}$ is N-element column vector as defined before, $\mathbf{Y} = \begin{bmatrix}Y_1,Y_2,...,Y_M\end{bmatrix}^T$ is an M-element column vector and $\mathbf{A}$ is a $M \times N$ matrix with
	
	$A_{ij} = a_{ij}$, $1 \leq i \leq M$, $1 \leq j \leq N$
	
	as its i,jth element
	
	Then $\mathbf{\boldsymbol{\mu}_Y} = E\{\mathbf{Y}\} = \mathbf{A}E\{\mathbf{X}\} = \mathbf{A}\mathbf{\boldsymbol{\mu}_X}$
	
	$\mathbf{\Sigma_Y} = E\{(\mathbf{Y} - \mathbf{\boldsymbol{\mu}_Y})(\mathbf{Y} - \mathbf{\boldsymbol{\mu}_Y})^T\} = \mathbf{A}\mathbf{\Sigma_X}\mathbf{A}^T$
	
	This vector notation is very convenient when dealing with multiple random variables.
	
	Let $A$ and $B$ be two events with individual probabilities $P(A)$ and $P(B)$. Let $P(AB)$ denote the probability of the intersection of $A$ and $B$. Then the conditional probabilities are defined as
	
	\begin{equation*}
		P(A|B) \left[\text{This is read as "Probability of A given B"}\right]
	\end{equation*}
	
	\begin{equation*}
		= \frac{P(AB)}{P(B)}
	\end{equation*}
	
	Similarly
	
	\begin{equation*}
		P(B|A) = \frac{P(AB)}{P(A)}
	\end{equation*}
	
	If the events $A$ and $B$ are statistically independent, $P(A|B) = P(A)$ and $P(B|A) = P(B)$.
	
	This is known as \underline{Bayes theorem} and is of great significance in pattern recognition.
\end{document}