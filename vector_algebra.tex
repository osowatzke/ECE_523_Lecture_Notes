\documentclass[fleqn]{article}
\usepackage[nodisplayskipstretch]{setspace}
\usepackage{amsmath, nccmath, bm}
\usepackage{amssymb}

\newcommand{\zerodisplayskip}{
	\setlength{\abovedisplayskip}{0pt}%
	\setlength{\belowdisplayskip}{0pt}%
	\setlength{\abovedisplayshortskip}{0pt}%
	\setlength{\belowdisplayshortskip}{0pt}%
	\setlength{\mathindent}{0pt}}
	
\title{Vector Algebra}
\author{}
\date{}

\setlength{\parindent}{0pt}

\begin{document}
	\offinterlineskip
	\setlength{\lineskip}{12pt}
	\zerodisplayskip
	\maketitle
	
	\section{Notation}
		
	Scalar: $x$

	Column Vector: $\mathbf{x} = \begin{bmatrix}x_1 & x_2 & \cdots & x_n \end{bmatrix}^T$
		
	Matrix ($m \times n$):
		
	\begin{equation*}
		\mathbf{A} = \begin{bmatrix}
			a_{11} & a_{12} & \cdots & a_{1n}\\
			a_{21} & a_{22} & \cdots & a_{2n}\\
			\vdots & \vdots & \ddots & \vdots\\
			a_{m1} & a_{m2} & \cdots & a_{mn}
		\end{bmatrix}
	\end{equation*}
		
	\begin{equation*}
		\text{Symmetric Matrix} \Leftrightarrow \mathbf{A}^T = \mathbf{A}
	\end{equation*}
			
	\begin{equation*}\
		\mathbf{A}^{\dag} = \text{Conjugate Tranpose of } \mathbf{A}
	\end{equation*}
	
	\section{Linear Independence}
	
	$\{\mathbf{x_1}, \mathbf{x_2},...,\mathbf{x_M}\}$ is a set of linearly independent vector provided
	
	\begin{equation*}
		a_1\mathbf{x_1} + a_2\mathbf{x_2} + \cdots + a_M\mathbf{x_M} = \mathbf{0} \Rightarrow a_1 = 0 = a_2 = \cdots = a_M 
	\end{equation*}
	
	From a set of $M$ linearly independent vectors, we can always construct $M$ orthonormal vectors.
	
	orthonormality $\Rightarrow$
	
	\begin{equation*}
		\langle \mathbf{x_i}, \mathbf{x_j} \rangle = \mathbf{x_i}^{\dag}\mathbf{x_j} = \delta_{ij} = \begin{cases}
			0 & \text{if}\ i \neq j \\
			1 & \text{if}\ i = j
		\end{cases}
	\end{equation*}
	
	\begin{equation*}
		\langle \mathbf{x_i}, \mathbf{x_j} \rangle = \text{Inner Product}
	\end{equation*}
		
	\begin{equation*}
		\langle \mathbf{x_i}, \mathbf{x_i} \rangle = \mathbf{x_i}^{\dag}\mathbf{x_i} = \text{Squared Norm of } \mathbf{x_i}
	\end{equation*}
	
	\begin{equation*}
		\text{Outer Product} = \rangle \mathbf{x}, \mathbf{y} \langle = \mathbf{x}\mathbf{y}^{\dag} \quad \text{(a Matrix)}
	\end{equation*}
	
	\section{Matrices}
	
	Matrix Multiplication
	
	Determinant of $\mathbf{A} = \text{Det}(\mathbf{A}) = |\mathbf{A}|$
	
	Matrix inverse $\mathbf{A}^{-1}$ exists if we have $\mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}$\quad (Identity Matrix)
	
	Determinant of $\mathbf{A} = 0 \Leftrightarrow \mathbf{A}$ is singular or non-invertible.
	
	An eigenvalue and eigenvector of an $n \times n$ matrix $\mathbf{A}$ satisfies
	
	\begin{equation*}
		\mathbf{A}\mathbf{{\Phi}_i} = {\lambda}_i\mathbf{{\Phi}_i},\quad i = 1,2,...,n
	\end{equation*}
	
	we can also find eigenvalues by solving the n-th order polynomial equation
	
	\begin{equation*}
		\text{det}(\mathbf{A} - \lambda\mathbf{I}) = 0\quad\text{for its n roots}
	\end{equation*}
	
	\begin{equation*}
		\text{Det of }\mathbf{A} = \prod_{i=1}^{n}{\lambda_i}
	\end{equation*}
	
	\begin{equation*}
		\text{Det of }\mathbf{A} = 0 \Leftrightarrow \text{At least one of }\lambda_i\text{ is zero}
	\end{equation*}
	
	\begin{equation*}
		\text{Trace of }\mathbf{A} \overset{\Delta}{=} \sum_{i=1}^{n}a_{ii} = \sum_{i=1}^{n}\lambda_i
	\end{equation*}
	
	Rank of a matrix $ = $ \# of nonzero $\lambda_i$'s.
	
	If $\mathbf{A}$ is symmetric, then $\lambda_i$'s are real.
	
	If $\mathbf{A}$ is symmetric and $\lambda_i \neq \lambda_j$, then $\mathbf{\Phi_i}^{\dag}\mathbf{\Phi_j} = 0$. Since eigenvectors can always be normalized to have unit norm, the eigenvectors of a $n \times n$ symmetric matrix $\mathbf{A}$ can form an \underline{orthornormal} basis set for n-dimensional vector space.
	
	condition number $ = \lambda_{\text{max}}/\lambda_{\text{min}}$
	
	\section{Positive Definite Matrix}
	
	$\mathbf{A}$ is positive definite $\Leftrightarrow$
	
	$\mathbf{x}^{\dag}\mathbf{A}\mathbf{x} \geq 0$ for all $\mathbf{x}$ and the equality holds iff $\mathbf{x} = \mathbf{0}$
	
	$\mathbf{A}$ is p.d. $\Leftrightarrow$ All $\lambda_i$'s are $ > 0$.
	
	Positive Semi-Definite $\Leftrightarrow \lambda_i \geq 0$
	
	Negative Definite $\Leftrightarrow \lambda_i < 0$
	
	Negative Semi-Definite $\Leftrightarrow \lambda_i \leq 0$
	\section{Vector Calculus}
	
	A scalar function $f(\cdot)$ of n variables $x_1,x_2,...,x_n$.
	
	$f(x_1,x_2,...,x_n) = f(\mathbf{x})$.
	
	\begin{equation*}
		\nabla_{\mathbf{x}}f = \frac{\delta f(\mathbf{x})}{\delta\mathbf{x}} = \begin{bmatrix}
			\frac{\delta f(\mathbf{x})}{\delta{x_1}} \\
			\vdots \\
			\frac{\delta f(\mathbf{x})}{\delta{x_n}}
		\end{bmatrix}
	\end{equation*}
	
	A vector function $\mathbf{f}(x_1,x_2,...,x_n) = \mathbf{f}(\mathbf{x})$ where $\mathbf{f}$ is $m \times 1$ and $\mathbf{x}$ is $n \times 1$
	
	\begin{equation*}
		\frac{\delta\mathbf{f}(\mathbf{x})}{\delta\mathbf{x}} = \begin{bmatrix}
			\frac{\delta{f_1}}{\delta{x_1}} & \frac{\delta{f_1}}{\delta{x_2}} & \cdots & \frac{\delta{f_1}}{\delta{x_n}} \\
			\vdots & \vdots & \ddots & \vdots \\
			\frac{\delta{f_m}}{\delta{x_1}} & \frac{\delta{f_m}}{\delta{x_2}} & \cdots & \frac{\delta{f_m}}{\delta{x_n}}
		\end{bmatrix}
	\end{equation*}
	
	\underline{Some useful results}
	
	\begin{equation*}
		\frac{d}{d\mathbf{x}}(\mathbf{Ax}) = \mathbf{A}
	\end{equation*}
	
	\begin{equation*}
		\frac{d}{d\mathbf{x}}(\mathbf{y}^T\mathbf{Ax}) = \mathbf{A}^T\mathbf{y}
	\end{equation*}
	
	\begin{equation*}
		\frac{d}{d\mathbf{x}}(\mathbf{x}^T\mathbf{y}) = \frac{d}{d\mathbf{x}}(\mathbf{y}^T\mathbf{x}) = \mathbf{y}
	\end{equation*}
	
	\begin{equation*}
		\frac{d}{d\mathbf{x}}(\mathbf{x}^T\mathbf{Ax}) = (\mathbf{A} + \mathbf{A}^T)\mathbf{x}
	\end{equation*}
	
	If $\mathbf{A}$ is symmetric,
	
	\begin{equation*}
		\frac{d}{d\mathbf{x}}(\mathbf{x}^T\mathbf{Ax}) = 2\mathbf{Ax}
	\end{equation*}
	
	\section{Taylor Series Expansion}
	
	\subsection{Scalar function}
	
	\begin{equation*}
		f(\mathbf{x}) = f(\mathbf{x_0}) + \left[\frac{df(\mathbf{x_0})}{d\mathbf{x}}\right]^T(\mathbf{x} - \mathbf{x_0}) + \frac{1}{2}(\mathbf{x}-\mathbf{x_0})^T\left[\frac{d^2f(\mathbf{x_0})}{d\mathbf{x}^2}\right](\mathbf{x} - \mathbf{x_0}) 
	\end{equation*}
	
	\begin{equation*}
		+ \text{higher order terms}
	\end{equation*}
	
	\subsection{Vector function}
	
	\begin{equation*}
		\mathbf{f}(\mathbf{x_0}) = \mathbf{f}(\mathbf{x_0}) + \left[\frac{d\mathbf{f}(\mathbf{x_0})}{d\mathbf{x}}\right](\mathbf{x} - \mathbf{x_0}) + \text{higher order terms}
	\end{equation*}
	
	\section{Least Squares Method}
	
	Suppose we have a system of linear equations where
	
	$\mathbf{Ax} = \mathbf{b}$
	
	where $\mathbf{A}$ is $m \times n$, $\mathbf{x}$ is $n \times 1$, and $\mathbf{b}$ is $m \times 1$
	
	Assume $m > n$ and rank of $\mathbf{A} = n$
	
	$\therefore$ we have more equations than unknowns. This is an overdetermined system.
	
	Least squares solution for $\mathbf{x}$ involves minimizing $J = \mathbf{e}^T\mathbf{e}$\quad (Squared Error)
	
	where $\mathbf{e} = (\mathbf{A\hat{x}} - \mathbf{b})$
	
	$\therefore J = (\mathbf{A\hat{x}} - \mathbf{b})^T(\mathbf{A\hat{x}} - \mathbf{b}) = \mathbf{\hat{x}}^T\mathbf{A}^T\mathbf{A}\mathbf{\hat{x}} - 2\mathbf{b}^T\mathbf{A\hat{x}} + \mathbf{b}^T\mathbf{b}$
	
	$\mathbf{A}^T\mathbf{A}$ is symmetric so
	
	\begin{equation*}
		\nabla_{\mathbf{\hat{x}}}J = 2\mathbf{A}^T\mathbf{A\hat{x}} - 2\mathbf{A}^T\mathbf{b} = \mathbf{0}
	\end{equation*}
	
	\begin{equation*}
		\Rightarrow \mathbf{\hat{x}_{LS}} = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b} = \mathbf{A}^{\dag}\mathbf{b}
	\end{equation*}
	
	\begin{equation*}
		\mathbf{A}^{\dag} = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T = \text{Pseudo inverse}
	\end{equation*}
	
	Note: $\mathbf{A}^{\dag}\mathbf{A} = \mathbf{I}$, but $\mathbf{A}\mathbf{A}^{\dag} \neq \mathbf{I}$
	
	If $\mathbf{A}$ is invertible, $\mathbf{A}^{\dag} = \mathbf{A}^{-1}\left(\mathbf{A}^T\right)^{-1}\mathbf{A}^T = \mathbf{A}^{-1}$
\end{document}